# name: test/sql/local/iceberg_catalog_read.test
# description: test integration with iceberg catalog read
# group: [iceberg]

require-env ICEBERG_SERVER_AVAILABLE

require parquet

require iceberg

require httpfs

statement ok
CREATE SECRET (
       TYPE ICEBERG,
       ENDPOINT 'http://127.0.0.1:8181'
  );


statement ok
CREATE SECRET (
    TYPE S3,
    KEY_ID 'admin',
    SECRET 'password',
    ENDPOINT '127.0.0.1:9000',
    URL_STYLE 'path',
    USE_SSL 0
  );


statement ok
ATTACH '' AS my_datalake (TYPE ICEBERG);

mode skip

query IIIIII
Show all tables;
----
my_datalake	default	pyspark_iceberg_table_v1	[__]	[INTEGER]	false
my_datalake	default	pyspark_iceberg_table_v2	[__]	[INTEGER]	false
my_datalake	default	table_more_deletes	[__]	[INTEGER]	false
my_datalake	default	table_partitioned	[__]	[INTEGER]	false
my_datalake	default	table_unpartitioned	[__]	[INTEGER]	false
my_datalake	default	table_with_deletes	[__]	[INTEGER]	false


# verify deletes

# TODO verify the catalog has deletes (rest catalog stores data differently from local catalog)
query I nosort results_1
select sum(l_suppkey), min(l_suppkey), max(l_suppkey) from my_datalake.default.table_with_deletes;

query I nosort results_1
select sum(l_suppkey), min(l_suppkey), max(l_suppkey) from read_parquet('data/generated/intermediates/spark-rest/table_with_deletes/last/data.parquet/*.parquet');


