# name: test/sql/local/iceberg_catalog_read.test
# description: test integration with iceberg catalog read
# group: [iceberg]

require-env DUCKDB_ICEBERG_HAVE_GENERATED_DATA

require parquet

require iceberg

statement ok
set enable_logging=true;

# TODO verify the catalog has deletes (rest catalog stores data differently from local catalog)
query I nosort results_1
select sum(l_suppkey), min(l_suppkey), max(l_suppkey) from ICEBERG_SCAN('data/generated/iceberg/spark-local/table_with_deletes');

query I nosort results_1
select sum(l_suppkey), min(l_suppkey), max(l_suppkey) from read_parquet('data/generated/intermediates/spark-local/table_with_deletes/last/data.parquet/*.parquet');

query I
select message::INT > 0 from duckdb_logs() where type like '%duckdb-iceberg.finalize_deletes%';
----
true

# Verify Deletes
# joins with a table that has deletes.
# deletes are on the probe side
query I nosort results_2
select l1_deletes.l_orderkey, count(*) count from
  ICEBERG_SCAN('data/generated/iceberg/spark-local/lineitem_sf1_deletes') l1_deletes,
  ICEBERG_SCAN('data/generated/iceberg/spark-local/lineitem_sf_01_no_deletes') l2_no_deletes
where l1_deletes.l_partkey = l2_no_deletes.l_partkey
group by l1_deletes.l_orderkey
order by l1_deletes.l_orderkey, count
limit 10;
----

# query I
# select message::INT > 0 from duckdb_logs() where type like '%duckdb-iceberg.finalize_deletes%' order by timestamp desc limit 1;
# ----
# true

query I nosort results_2
select l1_deletes.l_orderkey, count(*) count from
    read_parquet('data/generated/intermediates/spark-rest/lineitem_sf1_deletes/last/data.parquet/*.parquet') l1_deletes,
    read_parquet('data/generated/intermediates/spark-rest/lineitem_sf_01_no_deletes/last/data.parquet/*.parquet') l2_no_deletes
where l1_deletes.l_partkey = l2_no_deletes.l_partkey
group by l1_deletes.l_orderkey
order by l1_deletes.l_orderkey, count
limit 10;


# Verify a single delete
query IIII nosort result_3
select l_orderkey,
    l_partkey,
    l_suppkey,
    l_quantity
from ICEBERG_SCAN('data/generated/iceberg/spark-local/lineitem_sf_01_1_delete')
order by l_partkey, l_orderkey limit 10;
----

query IIII nosort result_3
select l_orderkey,
    l_partkey,
    l_suppkey,
    l_quantity
from read_parquet('data/generated/intermediates/spark-rest/lineitem_sf_01_1_delete/last/data.parquet/*.parquet')
order by l_partkey, l_orderkey limit 10;
----

query I
select count(*)
from ICEBERG_SCAN('data/generated/iceberg/spark-local/lineitem_sf_01_1_delete')
where l_orderkey=10053 and l_partkey = 77;
----
0

query I
select count(*)
from read_parquet('data/generated/intermediates/spark-rest/lineitem_sf_01_1_delete/last/data.parquet/*.parquet')
where l_orderkey=10053 and l_partkey = 77;
----
0


# verify paritioned table read
# add tests for partitioned tables.
query II nosort result_4
select l_shipmode, count(*) count
from ICEBERG_SCAN('data/generated/iceberg/spark-local/lineitem_partitioned_l_shipmode')
group by l_shipmode order by count;
----

query II nosort result_4
select l_shipmode, count(*) count
from read_parquet('data/generated/intermediates/spark-rest/lineitem_partitioned_l_shipmode/last/data.parquet/*.parquet')
group by l_shipmode order by count;
----

# verify delete from partitioned table
# create table lineitem_partitioned_mmany_deletes as select * from lineitem (merge on write, partition by l_shipmode)
# select count(*), l_shipmode from lineitem where l_linenumber in (3,4,5,6) group by l_shipmode ;
# add tests for partitioned tables.
query II nosort result_5
select l_shipmode, count(*) count
from ICEBERG_SCAN('data/generated/iceberg/spark-local/lineitem_partitioned_l_shipmode_deletes')
group by l_shipmode order by count;
----

query II nosort result_5
select l_shipmode, count(*) count
from read_parquet('data/generated/intermediates/spark-rest/lineitem_partitioned_l_shipmode_deletes/last/data.parquet/*.parquet')
group by l_shipmode order by count;
----

query I
select message::INT > 0
from duckdb_logs()
where type like '%duckdb-iceberg.finalize_deletes%'
order by timestamp desc limit 1;
----
1

